# AuthBridge Demo Deployment (with SPIFFE)
#
# This deployment demonstrates:
# 1. Agent pod with automatic client registration using SPIFFE ID
# 2. AuthProxy sidecar intercepts inbound requests (JWT validation) and outbound requests (HTTP: token exchange; HTTPS: TLS passthrough)
# 3. Auth Target (target server) validates the exchanged tokens
#
# Flow:
#   Inbound:  Client -> Envoy (validates JWT via ext proc) -> Agent app
#   Outbound HTTP:  Agent app -> Envoy (exchanges token via ext proc) -> Auth Target
#   Outbound HTTPS: Agent app -> Envoy (TLS passthrough via tcp_proxy) -> Auth Target
#
#   0. SPIFFE Helper obtains SVID (Agent's identity = SPIFFE ID)
#   1. Client Registration registers Agent with Keycloak
#   2. Caller gets token with aud: Agent's SPIFFE ID
#   3. Caller passes token to Agent
#   4. Envoy inbound listener validates JWT (signature + issuer via JWKS)
#   5. Agent calls Auth Target with Caller's token
#   6. Envoy outbound listener exchanges token (using Agent's credentials)
#   7. Auth Target validates exchanged token (aud: auth-target)
#
# Pod 1: Agent Pod
#   - Agent: The agent application (receives tokens from Callers)
#   - SPIFFE Helper: Provides SPIFFE credentials (SVID)
#   - Client Registration: Registers Agent with Keycloak using SPIFFE ID
#   - AuthProxy: Simple pass-through proxy (JWT validation handled by ext proc)
#   - Envoy: Intercepts inbound traffic (JWT validation) and outbound traffic (HTTP: token exchange; HTTPS: TLS passthrough)
#
# Pod 2: Auth Target (Target Server)
#   - Validates tokens with audience "auth-target"

---
# =============================================================================
# NAMESPACE AND SERVICE ACCOUNT
# =============================================================================
apiVersion: v1
kind: Namespace
metadata:
  name: authbridge

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: agent
  namespace: authbridge

---
# =============================================================================
# AUTH PROXY CONFIGURATION
# CLIENT_ID and CLIENT_SECRET come from /shared/ files (populated by client-registration)
# TARGET_AUDIENCE and TARGET_SCOPES define the target service for token exchange
# =============================================================================
apiVersion: v1
kind: Secret
metadata:
  name: auth-proxy-config
  namespace: authbridge
type: Opaque
stringData:
  TOKEN_URL: "http://keycloak-service.keycloak.svc.cluster.local:8080/realms/demo/protocol/openid-connect/token"
  ISSUER: "http://keycloak.localtest.me:8080/realms/demo"
  TARGET_AUDIENCE: "auth-target"
  TARGET_SCOPES: "openid auth-target-aud"

---
# =============================================================================
# CONFIGMAPS
# =============================================================================
# Shared ConfigMap for environment variables
apiVersion: v1
kind: ConfigMap
metadata:
  name: authbridge-config
  namespace: authbridge
data:
  SPIRE_ENABLED: "true"
  KEYCLOAK_URL: "http://keycloak-service.keycloak.svc:8080"
  KEYCLOAK_REALM: "demo"
  KEYCLOAK_ADMIN_USERNAME: "admin"
  KEYCLOAK_ADMIN_PASSWORD: "admin"
  KEYCLOAK_TOKEN_EXCHANGE_ENABLED: "true"
  KEYCLOAK_CLIENT_REGISTRATION_ENABLED: "true"

---
# SPIFFE Helper configuration for the caller pod
apiVersion: v1
kind: ConfigMap
metadata:
  name: spiffe-helper-config
  namespace: authbridge
data:
  helper.conf: |
    agent_address = "/spiffe-workload-api/spire-agent.sock"
    cmd = ""
    cmd_args = ""
    svid_file_name = "/opt/svid.pem"
    svid_key_file_name = "/opt/svid_key.pem"
    svid_bundle_file_name = "/opt/svid_bundle.pem"
    jwt_svids = [{jwt_audience="kagenti", jwt_svid_file_name="/opt/jwt_svid.token"}]
    jwt_svid_file_mode = 0644
    include_federated_domains = true

---
# Envoy configuration for AuthProxy
apiVersion: v1
kind: ConfigMap
metadata:
  name: envoy-config
  namespace: authbridge
data:
  envoy.yaml: |
    admin:
      address:
        socket_address:
          protocol: TCP
          address: 127.0.0.1
          port_value: 9901

    static_resources:
      listeners:
      - name: outbound_listener
        address:
          socket_address:
            protocol: TCP
            address: 0.0.0.0
            port_value: 15123
        listener_filters:
        - name: envoy.filters.listener.original_dst
          typed_config:
            "@type": type.googleapis.com/envoy.extensions.filters.listener.original_dst.v3.OriginalDst
        - name: envoy.filters.listener.tls_inspector
          typed_config:
            "@type": type.googleapis.com/envoy.extensions.filters.listener.tls_inspector.v3.TlsInspector
        filter_chains:
        # TLS passthrough: forward HTTPS traffic as-is via TCP proxy
        - filter_chain_match:
            transport_protocol: tls
          filters:
          - name: envoy.filters.network.tcp_proxy
            typed_config:
              "@type": type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxy
              stat_prefix: outbound_tls_passthrough
              cluster: original_destination
        # Plaintext HTTP: inspect and process via ext_proc
        - filter_chain_match:
            transport_protocol: raw_buffer
          filters:
          - name: envoy.filters.network.http_connection_manager
            typed_config:
              "@type": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager
              stat_prefix: outbound_http
              codec_type: AUTO
              route_config:
                name: outbound_routes
                virtual_hosts:
                - name: catch_all
                  domains: ["*"]
                  routes:
                  - match:
                      prefix: "/"
                    route:
                      cluster: original_destination
              http_filters:
              - name: envoy.filters.http.ext_proc
                typed_config:
                  "@type": type.googleapis.com/envoy.extensions.filters.http.ext_proc.v3.ExternalProcessor
                  grpc_service:
                    envoy_grpc:
                      cluster_name: ext_proc_cluster
                    timeout: 30s
                  processing_mode:
                    request_header_mode: SEND
                    response_header_mode: SKIP
                    request_body_mode: NONE
                    response_body_mode: NONE
              - name: envoy.filters.http.router
                typed_config:
                  "@type": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router

      - name: inbound_listener
        address:
          socket_address:
            protocol: TCP
            address: 0.0.0.0
            port_value: 15124
        listener_filters:
        - name: envoy.filters.listener.original_dst
          typed_config:
            "@type": type.googleapis.com/envoy.extensions.filters.listener.original_dst.v3.OriginalDst
        filter_chains:
        - filters:
          - name: envoy.filters.network.http_connection_manager
            typed_config:
              "@type": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager
              stat_prefix: inbound_http
              codec_type: AUTO
              route_config:
                name: inbound_routes
                virtual_hosts:
                - name: local_app
                  domains: ["*"]
                  request_headers_to_add:
                  - header:
                      key: "x-authbridge-direction"
                      value: "inbound"
                    append: false
                  routes:
                  - match:
                      prefix: "/"
                    route:
                      cluster: original_destination
              http_filters:
              - name: envoy.filters.http.ext_proc
                typed_config:
                  "@type": type.googleapis.com/envoy.extensions.filters.http.ext_proc.v3.ExternalProcessor
                  grpc_service:
                    envoy_grpc:
                      cluster_name: ext_proc_cluster
                    timeout: 30s
                  processing_mode:
                    request_header_mode: SEND
                    response_header_mode: SKIP
                    request_body_mode: NONE
                    response_body_mode: NONE
              - name: envoy.filters.http.router
                typed_config:
                  "@type": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router

      clusters:
      - name: ext_proc_cluster
        connect_timeout: 5s
        type: STATIC
        http2_protocol_options: {}
        lb_policy: ROUND_ROBIN
        load_assignment:
          cluster_name: ext_proc_cluster
          endpoints:
          - lb_endpoints:
            - endpoint:
                address:
                  socket_address:
                    address: 127.0.0.1
                    port_value: 9090

      - name: original_destination
        connect_timeout: 30s
        type: ORIGINAL_DST
        lb_policy: CLUSTER_PROVIDED

---
# =============================================================================
# AGENT POD
# Contains: Agent, SPIFFE Helper, Client Registration, AuthProxy
# =============================================================================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: agent
  namespace: authbridge
  labels:
    app: agent
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agent
  template:
    metadata:
      labels:
        app: agent
        # istio.io/dataplane-mode: none
      annotations:
        # ambient.istio.io/redirection: disabled
        sidecar.istio.io/inject: "false"
    spec:
      serviceAccountName: agent
      imagePullSecrets:
        - name: ghcr-secret
      initContainers:
        # Init container to set up iptables for traffic interception
        - name: proxy-init
          image: localhost/proxy-init:latest
          imagePullPolicy: IfNotPresent
          securityContext:
            privileged: true
            runAsNonRoot: false
            runAsUser: 0
          env:
            - name: PROXY_PORT
              value: "15123"
            - name: INBOUND_PROXY_PORT
              value: "15124"
            - name: PROXY_UID
              value: "1337"
            - name: OUTBOUND_PORTS_EXCLUDE
              value: "8080"  # Exclude Keycloak port from iptables redirect
          resources:
            limits:
              cpu: 10m
              memory: 10Mi
            requests:
              cpu: 10m
              memory: 10Mi
      containers:
        # Client registration - registers caller with Keycloak using SPIFFE ID
        # Runs as a container (not init) because it needs SPIFFE Helper to be running
        - name: client-registration
          image: ghcr.io/kagenti/kagenti-extensions/client-registration:latest
          command:
            - /bin/sh
            - -c
            - |
              echo "Waiting for SPIFFE credentials..."
              while [ ! -f /opt/jwt_svid.token ]; do
                echo "Waiting for SVID..."
                sleep 2
              done
              echo "SPIFFE credentials ready!"
              
              # Extract and save the client ID (SPIFFE ID) from the JWT
              # JWT format: header.payload.signature - we need the payload
              JWT_PAYLOAD=$(cat /opt/jwt_svid.token | cut -d'.' -f2)
              # Add padding if needed and decode
              CLIENT_ID=$(echo "$JWT_PAYLOAD"== | base64 -d 2>/dev/null | python -c "import sys,json; print(json.load(sys.stdin).get('sub',''))")
              echo "$CLIENT_ID" > /shared/client-id.txt
              echo "Client ID (SPIFFE ID): $CLIENT_ID"
              
              echo "Starting client registration..."
              python client_registration.py
              echo "Client registration complete!"
              echo "Staying alive to keep pod running..."
              tail -f /dev/null
          env:
            - name: SPIRE_ENABLED
              valueFrom:
                configMapKeyRef:
                  name: authbridge-config
                  key: SPIRE_ENABLED
            - name: KEYCLOAK_URL
              valueFrom:
                configMapKeyRef:
                  name: authbridge-config
                  key: KEYCLOAK_URL
            - name: KEYCLOAK_REALM
              valueFrom:
                configMapKeyRef:
                  name: authbridge-config
                  key: KEYCLOAK_REALM
            - name: KEYCLOAK_ADMIN_USERNAME
              valueFrom:
                configMapKeyRef:
                  name: authbridge-config
                  key: KEYCLOAK_ADMIN_USERNAME
            - name: KEYCLOAK_ADMIN_PASSWORD
              valueFrom:
                configMapKeyRef:
                  name: authbridge-config
                  key: KEYCLOAK_ADMIN_PASSWORD
            - name: KEYCLOAK_TOKEN_EXCHANGE_ENABLED
              valueFrom:
                configMapKeyRef:
                  name: authbridge-config
                  key: KEYCLOAK_TOKEN_EXCHANGE_ENABLED
            - name: KEYCLOAK_CLIENT_REGISTRATION_ENABLED
              valueFrom:
                configMapKeyRef:
                  name: authbridge-config
                  key: KEYCLOAK_CLIENT_REGISTRATION_ENABLED
            - name: CLIENT_NAME
              value: caller
            - name: SECRET_FILE_PATH
              value: /shared/client-secret.txt
          volumeMounts:
            - name: shared-data
              mountPath: /shared
            - name: svid-output
              mountPath: /opt
        # Agent - the agent application (receives tokens from Callers, calls external services)
        # Use: kubectl exec -it deployment/agent -n authbridge -c agent -- sh
        # Then: curl -H "Authorization: Bearer $TOKEN" http://auth-target-service:8081/test
        - name: agent
          image: nicolaka/netshoot:latest
          command:
            - /bin/sh
            - -c
            - |
              echo "Agent pod ready!"
              echo "Waiting for client registration to complete..."
              while [ ! -f /shared/client-secret.txt ] || [ ! -f /shared/client-id.txt ]; do
                sleep 2
              done
              
              CLIENT_ID=$(cat /shared/client-id.txt)
              CLIENT_SECRET=$(cat /shared/client-secret.txt)
              
              echo ""
              echo "============================================"
              echo "Agent registered with Keycloak!"
              echo "Client ID (Agent's SPIFFE ID): $CLIENT_ID"
              echo "Client Secret: $CLIENT_SECRET"
              echo "============================================"
              echo ""
              echo "=== AuthBridge Flow ==="
              echo "1. Caller gets token with aud: $CLIENT_ID (this Agent's identity)"
              echo "2. Caller passes token to Agent"
              echo "3. Agent calls Auth Target with Caller's token"
              echo "4. AuthProxy exchanges token for aud: auth-target"
              echo ""
              echo "To test (simulating a Caller providing a token):"
              echo ""
              echo "  # Get a token (as if Caller obtained it)"
              echo "  CLIENT_ID=\$(cat /shared/client-id.txt)"
              echo "  CLIENT_SECRET=\$(cat /shared/client-secret.txt)"
              echo "  TOKEN=\$(curl -sX POST http://keycloak-service.keycloak.svc:8080/realms/demo/protocol/openid-connect/token \\"
              echo "    -d 'grant_type=client_credentials' \\"
              echo "    -d \"client_id=\$CLIENT_ID\" \\"
              echo "    -d \"client_secret=\$CLIENT_SECRET\" | jq -r '.access_token')"
              echo ""
              echo "  # Verify token audience (should be Agent's SPIFFE ID)"
              echo "  echo \$TOKEN | cut -d'.' -f2 | base64 -d 2>/dev/null | jq '{aud, azp}'"
              echo ""
              echo "  # Agent calls auth-target (AuthProxy will exchange token)"
              echo "  curl -H \"Authorization: Bearer \$TOKEN\" http://auth-target-service:8081/test"
              echo ""
              # Keep container running
              tail -f /dev/null
          volumeMounts:
            - name: shared-data
              mountPath: /shared
        # SPIFFE Helper - provides SPIFFE credentials (SVID)
        - name: spiffe-helper
          image: ghcr.io/spiffe/spiffe-helper:nightly
          command:
            - /spiffe-helper
            - -config=/etc/spiffe-helper/helper.conf
            - run
          volumeMounts:
            - name: spiffe-helper-config
              mountPath: /etc/spiffe-helper
            - name: spire-agent-socket
              mountPath: /spiffe-workload-api
            - name: svid-output
              mountPath: /opt
        # AuthProxy - pass-through proxy (JWT validation handled by inbound ext proc)
        - name: auth-proxy
          image: localhost/auth-proxy:latest
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 8080
              name: http
          env:
            - name: TARGET_SERVICE_URL
              value: "http://auth-target-service:8081"
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 128Mi
        # Envoy proxy - intercepts inbound traffic (JWT validation) and outbound traffic (HTTP: token exchange; HTTPS: TLS passthrough)
        # Uses dynamic credentials from /shared/ (populated by client-registration)
        # The SPIFFE ID is both the client_id AND the token audience, enabling exchange
        - name: envoy-proxy
          image: localhost/envoy-with-processor:latest
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 1337
            runAsGroup: 1337
          ports:
            - containerPort: 15123
              name: envoy-outbound
            - containerPort: 15124
              name: envoy-inbound
            - containerPort: 9901
              name: envoy-admin
            - containerPort: 9090
              name: ext-proc
          env:
            # Token exchange configuration
            - name: TOKEN_URL
              valueFrom:
                secretKeyRef:
                  name: auth-proxy-config
                  key: TOKEN_URL
            # Issuer for inbound JWT validation (must match Keycloak frontend URL / iss claim)
            - name: ISSUER
              valueFrom:
                secretKeyRef:
                  name: auth-proxy-config
                  key: ISSUER
            - name: TARGET_AUDIENCE
              valueFrom:
                secretKeyRef:
                  name: auth-proxy-config
                  key: TARGET_AUDIENCE
            - name: TARGET_SCOPES
              valueFrom:
                secretKeyRef:
                  name: auth-proxy-config
                  key: TARGET_SCOPES
            # Dynamic credentials from client-registration (via files)
            - name: CLIENT_ID_FILE
              value: "/shared/client-id.txt"
            - name: CLIENT_SECRET_FILE
              value: "/shared/client-secret.txt"
          volumeMounts:
            - name: envoy-config
              mountPath: /etc/envoy
              readOnly: true
            - name: shared-data
              mountPath: /shared
              readOnly: true
          resources:
            limits:
              cpu: 200m
              memory: 256Mi
            requests:
              cpu: 50m
              memory: 64Mi
      volumes:
        - name: shared-data
          emptyDir: {}
        - name: spiffe-helper-config
          configMap:
            name: spiffe-helper-config
        - name: spire-agent-socket
          hostPath:
            path: /run/spire/agent-sockets
        - name: svid-output
          emptyDir: {}
        - name: envoy-config
          configMap:
            name: envoy-config

---
# =============================================================================
# AUTH TARGET POD (Target Server)
# Validates tokens with audience "auth-target"
# =============================================================================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: auth-target
  namespace: authbridge
  labels:
    app: auth-target
spec:
  replicas: 1
  selector:
    matchLabels:
      app: auth-target
  template:
    metadata:
      labels:
        app: auth-target
    spec:
      containers:
        - name: auth-target
          image: ghcr.io/kagenti/kagenti-extensions/demo-app:latest
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 8081
          env:
            - name: ISSUER
              value: "http://keycloak.localtest.me:8080/realms/demo"
            - name: JWKS_URL
              value: "http://keycloak-service.keycloak.svc.cluster.local:8080/realms/demo/protocol/openid-connect/certs"
            # Auth Target expects tokens with "auth-target" audience
            - name: AUDIENCE
              value: "auth-target"
          resources:
            requests:
              memory: "64Mi"
              cpu: "250m"
            limits:
              memory: "128Mi"
              cpu: "500m"

---
# Auth Target Service
apiVersion: v1
kind: Service
metadata:
  name: auth-target-service
  namespace: authbridge
  labels:
    app: auth-target
spec:
  type: ClusterIP
  ports:
    - port: 8081
      targetPort: 8081
      protocol: TCP
      name: http
  selector:
    app: auth-target
